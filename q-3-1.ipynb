{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix ,accuracy_score\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"wine-quality/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =df.drop(['quality'],axis=1)\n",
    "y=df['quality']\n",
    "X = (X - X.mean())/X.std()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "intrain=X_train\n",
    "intest=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(thetaT):\n",
    "    return 1/(1+np.exp(-thetaT))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data=pd.concat([X_train,y_train],axis=1)\n",
    "X=X_train\n",
    "ones = np.ones([X.shape[0],1])\n",
    "X = np.concatenate((ones,X),axis=1)\n",
    "y=pd.DataFrame(y_train)\n",
    "y=y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,iters,alpha):\n",
    "    for i in range(iters):\n",
    "        theta = theta - (alpha/len(X)) * np.sum(X * (grad(X @ theta.T) - y), axis=0)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*considering one class at a time rest all as zero and applying gradient descent to get theta for it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "betaList=[]\n",
    "count=[]\n",
    "for j in range(0,11):\n",
    "    y_temp=[]\n",
    "    X1=copy.deepcopy(X)\n",
    "    r=0;\n",
    "    for i in range(len(y)):\n",
    "        if y[i]==j:\n",
    "            r=r+1\n",
    "            y_temp.append(1)\n",
    "        else:\n",
    "            y_temp.append(0)\n",
    "       \n",
    "    y_temp1=pd.DataFrame(y_temp)\n",
    "    y_temp2=y_temp1.values\n",
    "    theta = np.zeros([1,12])\n",
    "    alpha = 0.01\n",
    "    iters = 1000\n",
    "    count.append(r)\n",
    "    g = gradientDescent(X1,y_temp2,theta,iters,alpha)\n",
    "    betaList.append(g[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculating y for each theta and the class whose theta value will result in maximum value(here maximum value is not its value or result value) it is probablity will be resultant class for that row*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for index,row in X_test.iterrows():\n",
    "    max1=0\n",
    "    row=list(row)\n",
    "    class1=0\n",
    "    for i in range(0,11):\n",
    "        y1=0\n",
    "        for j in range(1,12):\n",
    "            y1=y1+betaList[i][j]*row[j-1]\n",
    "\n",
    "        y1=y1+betaList[i][0]\n",
    "        y1=grad(y1)\n",
    "        if(y1>=max1):\n",
    "            max1=y1\n",
    "            class1=i\n",
    "    y_pred.append(class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   2   0   0   0   0]\n",
      " [  0   0  16   4   1   0   0]\n",
      " [  0   0 123 132   1   0   0]\n",
      " [  0   0  82 322   6   0   0]\n",
      " [  0   0  11 149   7   0   0]\n",
      " [  0   0   1  21   3   0   0]\n",
      " [  0   0   0   0   1   0   0]]\n",
      "Accuracy:  0.5124716553287982\n",
      "0.5124716553287982\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(\"Accuracy: \",accuracy_score(y_test,y_pred))\n",
    "print((y_test==y_pred).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "model = LogisticRegression(solver = 'lbfgs',multi_class='multinomial',max_iter=10000)\n",
    "intrain1 = StandardScaler().fit_transform(intrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(intrain1, y_train)\n",
    "intest1=StandardScaler().fit_transform(intest)\n",
    "y_pred = model.predict(intest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Accuracy : 0.54\n"
     ]
    }
   ],
   "source": [
    "count_misclassified = (y_test != y_pred).sum()\n",
    "# print('Misclassified samples: {}'.format(count_misclassified))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('System Accuracy : {:.2f}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
