{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "eps=np.finfo(float).eps\n",
    "from binarytree import tree,Node\n",
    "from sklearn.metrics import classification_report, confusion_matrix ,accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from operator import itemgetter\n",
    "import copy\n",
    "import collections\n",
    "from pylab import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA as PCA\n",
    "import sys\n",
    "import copy\n",
    "from sklearn import metrics\n",
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    varianceNum=0.0\n",
    "    for x in numbers:\n",
    "        varianceNum=varianceNum+pow(x-avg,2)\n",
    "    variance=varianceNum/float(len(numbers)-1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "df=pd.read_csv(\"intrusion_detection/data.csv\")\n",
    "df\n",
    "df1=copy.deepcopy(df)\n",
    "col=list(df)\n",
    "mean1={}\n",
    "for i in col:\n",
    "    if i!=\"xAttack\":\n",
    "        mean1[i]=mean(df[i])\n",
    "std1={}\n",
    "for i in col:\n",
    "    if i!=\"xAttack\":\n",
    "        std1[i]=stdev(df[i])\n",
    "for i in col:\n",
    "    if i!='xAttack':\n",
    "        df[i]=(df[i]-mean1[i])/std1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:29].values\n",
    "y = df.iloc[:,29].values\n",
    "X_std=X\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "eig_pairs=[]\n",
    "for i in range(len(eig_vals)):\n",
    "    t=(np.abs(eig_vals[i]), eig_vecs[:,i])\n",
    "    eig_pairs.append(t)\n",
    "total=0\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "for i in eig_pairs:\n",
    "    total+=i[0]    \n",
    "error=0\n",
    "j=0;\n",
    "arr=[]\n",
    "for i in eig_pairs:\n",
    "    error+=((i[0]/total)*100)\n",
    "    arr.append(i[1])\n",
    "    print \n",
    "    j=j+1;\n",
    "    if(error>90):\n",
    "        break;\n",
    "arr=np.array(arr)\n",
    "X1=X_std.dot(arr.T)\n",
    "X1\n",
    "l=[];\n",
    "x=len(X1);\n",
    "# print(x)\n",
    "for i in range(0,x):\n",
    "    l.append(i)\n",
    "Df = pd.DataFrame(data = X1,\n",
    "                  index=l)\n",
    "data=Df.values\n",
    "k=5\n",
    "c = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    contingency_matrix =metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GMM VS KMEANS**\n",
    "* GMM is a lot more flexible in terms of cluster covariance.\n",
    "* k-means is actually a special case of GMM in which each cluster’s covariance along all dimensions approaches 0.   This implies that a point will get assigned only to the cluster closest to it.\n",
    "* With GMM, each cluster can have unconstrained covariance structure. Think of rotated and/or elongated   distribution of points in a cluster, instead of spherical as in kmeans. As a result, cluster assignment is much   more flexible in GMM than in k-means\n",
    "*In kmeans, a point belongs to one and only one cluster, whereas in GMM a point belongs to each cluster to a different degree. The degree is based on the probability of the point being generated from each cluster’s (multivariate) normal distribution, with cluster center as the distribution’s mean and cluster covariance as its covariance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Gaussian Purity Score------------------\n",
      "0.7969723670945361\n"
     ]
    }
   ],
   "source": [
    "gm=mixture.GaussianMixture(n_components=5).fit(data)\n",
    "labels = gm.predict(data)\n",
    "print(\"----------Gaussian Purity Score------------------\")\n",
    "print(purity_score(y,labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
